{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8a5e19-f099-4bed-9e99-3dba5048f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3efe23-1821-4fd5-bb5b-f014cd49fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로드 및 처리\n",
    "data = read_csv('price data.csv', sep=',')\n",
    "xy = data[['avgTemp', 'minTemp', 'maxTemp', 'rainFall', 'avgPrice']].to_numpy()\n",
    "\n",
    "x_data = xy[:, :-1]  # 평균 기온, 최저 기온, 최고 기온, 강수량\n",
    "y_data = xy[:, [-1]]  # 평균 가격"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0521dddf-9916-4362-97dc-37ec22299eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "scaler = StandardScaler()\n",
    "x_data = scaler.fit_transform(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb83f652-b92c-4cb9-ac07-88c02d9cb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 \n",
    "x_data = x_data.astype(np.float64)\n",
    "y_data = y_data.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add1de87-43bf-4730-8d0e-3704007bf847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 편향 초기화\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "W = tf.Variable(initializer([4, 1], dtype=tf.float64), name=\"weight\")\n",
    "b = tf.Variable(tf.zeros([1], dtype=tf.float64), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd735d8-22ee-4987-af11-863225499d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가설 설정\n",
    "def hypothesis(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "# 손실 함수\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "def cost_fn(X, Y):\n",
    "    return mse(Y, hypothesis(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d3e87b-6d96-4a97-ac4a-9beafbed8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 설정 (0.0001)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5bf75e-e681-40f3-b9d0-c752b5d5e139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 6195025.5, 예측 값: [51.00153499]\n",
      "Step 500, Loss: 186909.078125, 예측 값: [2490.01937187]\n",
      "Step 1000, Loss: 186898.0625, 예측 값: [2490.25356906]\n",
      "Step 1500, Loss: 186896.390625, 예측 값: [2490.23696566]\n",
      "Step 2000, Loss: 186895.984375, 예측 값: [2490.1682597]\n",
      "Step 2500, Loss: 186895.75, 예측 값: [2490.08350861]\n",
      "Step 3000, Loss: 186895.578125, 예측 값: [2489.99567656]\n",
      "Step 3500, Loss: 186895.390625, 예측 값: [2489.90934381]\n",
      "Step 4000, Loss: 186895.25, 예측 값: [2489.8260534]\n",
      "Step 4500, Loss: 186895.109375, 예측 값: [2489.74625274]\n",
      "Step 5000, Loss: 186895.0, 예측 값: [2489.6699933]\n",
      "Step 5500, Loss: 186894.859375, 예측 값: [2489.59718939]\n",
      "Step 6000, Loss: 186894.765625, 예측 값: [2489.52771174]\n",
      "Step 6500, Loss: 186894.6875, 예측 값: [2489.46141728]\n",
      "Step 7000, Loss: 186894.578125, 예측 값: [2489.39816311]\n",
      "Step 7500, Loss: 186894.5, 예측 값: [2489.33781132]\n",
      "Step 8000, Loss: 186894.4375, 예측 값: [2489.28022947]\n",
      "Step 8500, Loss: 186894.34375, 예측 값: [2489.22529061]\n",
      "Step 9000, Loss: 186894.296875, 예측 값: [2489.17287349]\n",
      "Step 9500, Loss: 186894.25, 예측 값: [2489.12286222]\n",
      "Step 10000, Loss: 186894.1875, 예측 값: [2489.07514641]\n",
      "Step 10500, Loss: 186894.15625, 예측 값: [2489.02962055]\n",
      "Step 11000, Loss: 186894.109375, 예측 값: [2488.98618381]\n",
      "Step 11500, Loss: 186894.0625, 예측 값: [2488.94474153]\n",
      "Step 12000, Loss: 186894.015625, 예측 값: [2488.90520064]\n",
      "Step 12500, Loss: 186894.0, 예측 값: [2488.8674753]\n",
      "Step 13000, Loss: 186893.96875, 예측 값: [2488.83148069]\n",
      "Step 13500, Loss: 186893.953125, 예측 값: [2488.79713874]\n",
      "Step 14000, Loss: 186893.921875, 예측 값: [2488.76437332]\n",
      "Step 14500, Loss: 186893.9375, 예측 값: [2488.73311178]\n",
      "Step 15000, Loss: 186893.90625, 예측 값: [2488.70328543]\n",
      "Step 15500, Loss: 186893.875, 예측 값: [2488.6748272]\n",
      "Step 16000, Loss: 186893.859375, 예측 값: [2488.64767536]\n",
      "Step 16500, Loss: 186893.84375, 예측 값: [2488.62177018]\n",
      "Step 17000, Loss: 186893.828125, 예측 값: [2488.59705358]\n",
      "Step 17500, Loss: 186893.828125, 예측 값: [2488.57347151]\n",
      "Step 18000, Loss: 186893.8125, 예측 값: [2488.55097205]\n",
      "Step 18500, Loss: 186893.8125, 예측 값: [2488.52950445]\n",
      "Step 19000, Loss: 186893.765625, 예측 값: [2488.50902262]\n",
      "Step 19500, Loss: 186893.765625, 예측 값: [2488.48948139]\n",
      "Step 20000, Loss: 186893.765625, 예측 값: [2488.47083752]\n",
      "Step 20500, Loss: 186893.765625, 예측 값: [2488.45304783]\n",
      "Step 21000, Loss: 186893.75, 예측 값: [2488.43607549]\n",
      "Step 21500, Loss: 186893.734375, 예측 값: [2488.41988251]\n",
      "Step 22000, Loss: 186893.75, 예측 값: [2488.40443232]\n",
      "Step 22500, Loss: 186893.734375, 예측 값: [2488.38969149]\n",
      "Step 23000, Loss: 186893.734375, 예측 값: [2488.37562675]\n",
      "Step 23500, Loss: 186893.71875, 예측 값: [2488.36220818]\n",
      "Step 24000, Loss: 186893.71875, 예측 값: [2488.34940561]\n",
      "Step 24500, Loss: 186893.703125, 예측 값: [2488.33719088]\n",
      "Step 25000, Loss: 186893.734375, 예측 값: [2488.3255352]\n",
      "Step 25500, Loss: 186893.71875, 예측 값: [2488.31441439]\n",
      "Step 26000, Loss: 186893.71875, 예측 값: [2488.30380709]\n",
      "Step 26500, Loss: 186893.703125, 예측 값: [2488.29368395]\n",
      "Step 27000, Loss: 186893.703125, 예측 값: [2488.28402614]\n",
      "Step 27500, Loss: 186893.6875, 예측 값: [2488.27481118]\n",
      "Step 28000, Loss: 186893.703125, 예측 값: [2488.26601982]\n",
      "Step 28500, Loss: 186893.703125, 예측 값: [2488.25763247]\n",
      "Step 29000, Loss: 186893.71875, 예측 값: [2488.24962898]\n",
      "Step 29500, Loss: 186893.703125, 예측 값: [2488.24199298]\n",
      "Step 30000, Loss: 186893.703125, 예측 값: [2488.23470819]\n",
      "Step 30500, Loss: 186893.703125, 예측 값: [2488.22775707]\n",
      "Step 31000, Loss: 186893.703125, 예측 값: [2488.22112589]\n",
      "Step 31500, Loss: 186893.703125, 예측 값: [2488.21479829]\n",
      "Step 32000, Loss: 186893.703125, 예측 값: [2488.20876076]\n",
      "Step 32500, Loss: 186893.6875, 예측 값: [2488.20300111]\n",
      "Step 33000, Loss: 186893.6875, 예측 값: [2488.19750481]\n",
      "Step 33500, Loss: 186893.6875, 예측 값: [2488.19226309]\n",
      "Step 34000, Loss: 186893.703125, 예측 값: [2488.18725975]\n",
      "Step 34500, Loss: 186893.671875, 예측 값: [2488.1824869]\n",
      "Step 35000, Loss: 186893.6875, 예측 값: [2488.17793222]\n",
      "Step 35500, Loss: 186893.703125, 예측 값: [2488.17358795]\n",
      "Step 36000, Loss: 186893.703125, 예측 값: [2488.16944313]\n",
      "Step 36500, Loss: 186893.703125, 예측 값: [2488.16548717]\n",
      "Step 37000, Loss: 186893.703125, 예측 값: [2488.16171353]\n",
      "Step 37500, Loss: 186893.6875, 예측 값: [2488.15811272]\n",
      "Step 38000, Loss: 186893.6875, 예측 값: [2488.15467725]\n",
      "Step 38500, Loss: 186893.703125, 예측 값: [2488.15139966]\n",
      "Step 39000, Loss: 186893.6875, 예측 값: [2488.14827335]\n",
      "Step 39500, Loss: 186893.6875, 예측 값: [2488.14529008]\n",
      "Step 40000, Loss: 186893.6875, 예측 값: [2488.1424427]\n",
      "Step 40500, Loss: 186893.6875, 예측 값: [2488.13972693]\n",
      "Step 41000, Loss: 186893.6875, 예측 값: [2488.13713626]\n",
      "Step 41500, Loss: 186893.6875, 예측 값: [2488.13466301]\n",
      "Step 42000, Loss: 186893.6875, 예측 값: [2488.13230184]\n",
      "Step 42500, Loss: 186893.703125, 예측 값: [2488.13005391]\n",
      "Step 43000, Loss: 186893.6875, 예측 값: [2488.12790387]\n",
      "Step 43500, Loss: 186893.6875, 예측 값: [2488.1258595]\n",
      "Step 44000, Loss: 186893.6875, 예측 값: [2488.1239035]\n",
      "Step 44500, Loss: 186893.6875, 예측 값: [2488.12203824]\n",
      "Step 45000, Loss: 186893.6875, 예측 값: [2488.12025838]\n",
      "Step 45500, Loss: 186893.6875, 예측 값: [2488.1185595]\n",
      "Step 46000, Loss: 186893.671875, 예측 값: [2488.11694074]\n",
      "Step 46500, Loss: 186893.671875, 예측 값: [2488.11539468]\n",
      "Step 47000, Loss: 186893.6875, 예측 값: [2488.11392113]\n",
      "Step 47500, Loss: 186893.6875, 예측 값: [2488.1125158]\n",
      "Step 48000, Loss: 186893.6875, 예측 값: [2488.11117329]\n",
      "Step 48500, Loss: 186893.671875, 예측 값: [2488.10989176]\n",
      "Step 49000, Loss: 186893.6875, 예측 값: [2488.1086683]\n",
      "Step 49500, Loss: 186893.6875, 예측 값: [2488.10750372]\n",
      "Step 50000, Loss: 186893.6875, 예측 값: [2488.10639398]\n",
      "Step 50500, Loss: 186893.671875, 예측 값: [2488.10533079]\n",
      "Step 51000, Loss: 186893.6875, 예측 값: [2488.1043166]\n",
      "Step 51500, Loss: 186893.6875, 예측 값: [2488.10334981]\n",
      "Step 52000, Loss: 186893.6875, 예측 값: [2488.1024277]\n",
      "Step 52500, Loss: 186893.6875, 예측 값: [2488.10155002]\n",
      "Step 53000, Loss: 186893.6875, 예측 값: [2488.10071334]\n",
      "Step 53500, Loss: 186893.671875, 예측 값: [2488.09990881]\n",
      "Step 54000, Loss: 186893.6875, 예측 값: [2488.09914643]\n",
      "Step 54500, Loss: 186893.6875, 예측 값: [2488.09841931]\n",
      "Step 55000, Loss: 186893.6875, 예측 값: [2488.09772023]\n",
      "Step 55500, Loss: 186893.671875, 예측 값: [2488.09705969]\n",
      "Step 56000, Loss: 186893.6875, 예측 값: [2488.09642901]\n",
      "Step 56500, Loss: 186893.6875, 예측 값: [2488.09582323]\n",
      "Step 57000, Loss: 186893.6875, 예측 값: [2488.09524663]\n",
      "Step 57500, Loss: 186893.6875, 예측 값: [2488.09469639]\n",
      "Step 58000, Loss: 186893.6875, 예측 값: [2488.09417239]\n",
      "Step 58500, Loss: 186893.671875, 예측 값: [2488.09367287]\n",
      "Step 59000, Loss: 186893.671875, 예측 값: [2488.09319368]\n",
      "Step 59500, Loss: 186893.6875, 예측 값: [2488.09273712]\n",
      "Step 60000, Loss: 186893.703125, 예측 값: [2488.09230395]\n",
      "Step 60500, Loss: 186893.703125, 예측 값: [2488.09188871]\n",
      "Step 61000, Loss: 186893.6875, 예측 값: [2488.09149123]\n",
      "Step 61500, Loss: 186893.703125, 예측 값: [2488.09111824]\n",
      "Step 62000, Loss: 186893.703125, 예측 값: [2488.09075775]\n",
      "Step 62500, Loss: 186893.703125, 예측 값: [2488.09041231]\n",
      "Step 63000, Loss: 186893.703125, 예측 값: [2488.09008421]\n",
      "Step 63500, Loss: 186893.703125, 예측 값: [2488.089774]\n",
      "Step 64000, Loss: 186893.703125, 예측 값: [2488.08947254]\n",
      "Step 64500, Loss: 186893.703125, 예측 값: [2488.08919137]\n",
      "Step 65000, Loss: 186893.6875, 예측 값: [2488.08891799]\n",
      "Step 65500, Loss: 186893.703125, 예측 값: [2488.08866114]\n",
      "Step 66000, Loss: 186893.703125, 예측 값: [2488.08841004]\n",
      "Step 66500, Loss: 186893.703125, 예측 값: [2488.08817568]\n",
      "Step 67000, Loss: 186893.703125, 예측 값: [2488.08795119]\n",
      "Step 67500, Loss: 186893.703125, 예측 값: [2488.08773365]\n",
      "Step 68000, Loss: 186893.6875, 예측 값: [2488.08752999]\n",
      "Step 68500, Loss: 186893.6875, 예측 값: [2488.08733237]\n",
      "Step 69000, Loss: 186893.703125, 예측 값: [2488.08714573]\n",
      "Step 69500, Loss: 186893.703125, 예측 값: [2488.0869681]\n",
      "Step 70000, Loss: 186893.703125, 예측 값: [2488.08679766]\n",
      "Step 70500, Loss: 186893.703125, 예측 값: [2488.08663779]\n",
      "Step 71000, Loss: 186893.703125, 예측 값: [2488.08648088]\n",
      "Step 71500, Loss: 186893.703125, 예측 값: [2488.08633338]\n",
      "Step 72000, Loss: 186893.703125, 예측 값: [2488.08619719]\n",
      "Step 72500, Loss: 186893.703125, 예측 값: [2488.08606087]\n",
      "Step 73000, Loss: 186893.6875, 예측 값: [2488.08593579]\n",
      "Step 73500, Loss: 186893.703125, 예측 값: [2488.08581056]\n",
      "Step 74000, Loss: 186893.703125, 예측 값: [2488.08569429]\n",
      "Step 74500, Loss: 186893.6875, 예측 값: [2488.08558451]\n",
      "Step 75000, Loss: 186893.6875, 예측 값: [2488.08547666]\n",
      "Step 75500, Loss: 186893.703125, 예측 값: [2488.08537749]\n",
      "Step 76000, Loss: 186893.6875, 예측 값: [2488.0852768]\n",
      "Step 76500, Loss: 186893.6875, 예측 값: [2488.08518583]\n",
      "Step 77000, Loss: 186893.6875, 예측 값: [2488.0850985]\n",
      "Step 77500, Loss: 186893.6875, 예측 값: [2488.08501575]\n",
      "Step 78000, Loss: 186893.6875, 예측 값: [2488.084935]\n",
      "Step 78500, Loss: 186893.6875, 예측 값: [2488.08485759]\n",
      "Step 79000, Loss: 186893.703125, 예측 값: [2488.08478303]\n",
      "Step 79500, Loss: 186893.703125, 예측 값: [2488.08471639]\n",
      "Step 80000, Loss: 186893.6875, 예측 값: [2488.08465298]\n",
      "Step 80500, Loss: 186893.6875, 예측 값: [2488.08458781]\n",
      "Step 81000, Loss: 186893.703125, 예측 값: [2488.08452751]\n",
      "Step 81500, Loss: 186893.703125, 예측 값: [2488.08446875]\n",
      "Step 82000, Loss: 186893.703125, 예측 값: [2488.08441234]\n",
      "Step 82500, Loss: 186893.703125, 예측 값: [2488.08435872]\n",
      "Step 83000, Loss: 186893.6875, 예측 값: [2488.08430904]\n",
      "Step 83500, Loss: 186893.6875, 예측 값: [2488.08426401]\n",
      "Step 84000, Loss: 186893.671875, 예측 값: [2488.08421728]\n",
      "Step 84500, Loss: 186893.6875, 예측 값: [2488.08417542]\n",
      "Step 85000, Loss: 186893.6875, 예측 값: [2488.08413411]\n",
      "Step 85500, Loss: 186893.6875, 예측 값: [2488.08409459]\n",
      "Step 86000, Loss: 186893.6875, 예측 값: [2488.08405632]\n",
      "Step 86500, Loss: 186893.6875, 예측 값: [2488.08401907]\n",
      "Step 87000, Loss: 186893.6875, 예측 값: [2488.08398363]\n",
      "Step 87500, Loss: 186893.6875, 예측 값: [2488.08395091]\n",
      "Step 88000, Loss: 186893.6875, 예측 값: [2488.08391978]\n",
      "Step 88500, Loss: 186893.6875, 예측 값: [2488.08388956]\n",
      "Step 89000, Loss: 186893.6875, 예측 값: [2488.08386011]\n",
      "Step 89500, Loss: 186893.6875, 예측 값: [2488.08383225]\n",
      "Step 90000, Loss: 186893.6875, 예측 값: [2488.08380468]\n",
      "Step 90500, Loss: 186893.6875, 예측 값: [2488.08378019]\n",
      "Step 91000, Loss: 186893.703125, 예측 값: [2488.08375593]\n",
      "Step 91500, Loss: 186893.703125, 예측 값: [2488.08373426]\n",
      "Step 92000, Loss: 186893.703125, 예측 값: [2488.08371397]\n",
      "Step 92500, Loss: 186893.703125, 예측 값: [2488.08369479]\n",
      "Step 93000, Loss: 186893.703125, 예측 값: [2488.08367433]\n",
      "Step 93500, Loss: 186893.703125, 예측 값: [2488.08365655]\n",
      "Step 94000, Loss: 186893.703125, 예측 값: [2488.08363861]\n",
      "Step 94500, Loss: 186893.703125, 예측 값: [2488.08362388]\n",
      "Step 95000, Loss: 186893.703125, 예측 값: [2488.08360678]\n",
      "Step 95500, Loss: 186893.703125, 예측 값: [2488.0835892]\n",
      "Step 96000, Loss: 186893.703125, 예측 값: [2488.08357218]\n",
      "Step 96500, Loss: 186893.703125, 예측 값: [2488.08355833]\n",
      "Step 97000, Loss: 186893.703125, 예측 값: [2488.08354454]\n",
      "Step 97500, Loss: 186893.703125, 예측 값: [2488.08353285]\n",
      "Step 98000, Loss: 186893.703125, 예측 값: [2488.08352131]\n",
      "Step 98500, Loss: 186893.703125, 예측 값: [2488.08351025]\n",
      "Step 99000, Loss: 186893.703125, 예측 값: [2488.08349977]\n",
      "Step 99500, Loss: 186893.703125, 예측 값: [2488.08348992]\n",
      "Step 100000, Loss: 186893.703125, 예측 값: [2488.08347961]\n",
      "학습된 모델을 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "# 학습 \n",
    "for step in range(100001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = cost_fn(x_data, y_data)\n",
    "    \n",
    "    gradients = tape.gradient(cost, [W, b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Step {step}, Loss: {cost.numpy()}, 예측 값: {hypothesis(x_data).numpy()[0]}\")\n",
    "\n",
    "# 저장\n",
    "ckpt = tf.train.Checkpoint(W=W, b=b)\n",
    "ckpt.save(\"./saved.ckpt\")\n",
    "print(\"학습된 모델을 저장했습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
